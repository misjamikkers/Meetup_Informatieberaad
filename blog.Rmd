---
title: "Meetup blog"
author: "Misja Mikkers & Gertjan Verhoeven"
output:
  html_document:
    df_print: paged
---





```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

```


```{r}
# Packages
library(tidyverse)
library(dagitty)
library(caret)
library(ranger)
library(stargazer)
library(knitr)
library(ggrepel)
library(randomForest)
```


# Inleiding

Het doel van dit blog is om een toegankelijke inleiding te geven over causaliteit en data analyse. Dit blog is geschreven naar aanleiding van een uitnodiging om een workshop te organiseren voor de 7de Meet Up van het Informatieberaad Zorg op 23 september 2019. Het publiek van deze Meet Ups is zeer divers: patiënten, zorgprofessionals, beleidsmakers én leveranciers en inkopers van technische oplossingen voor de zorg zijn bij deze Meet Ups aanwezig.

[Gertjan Verhoeven](https://gsverhoeven.github.io/#about) en ik hebben de presentatie tijdens de Meet Up omgezet naar dit blog.

Het voorspellen van fenomenen met behulp machine learning is de afgelopen jaren een groot succes gebleken. Grote hoeveelheden data en toegenomen computerkracht hebben geleid tot veel ontwikkelingen en toepassingen op het gebied van voorspellen. Voorbeelden zijn het real time voorspellen van credit card fraude, het systeem van aanbevelingen bij bedrijven als Spotify, Bol, Netflix etc. Deze modellen hebben gemeen dat ze zich richten op het voorspellen van fenomenen, maar niet gericht zijn op causaliteit. Als de wereld verandert, dan vermindert de voorspellende waarde van het model en wordt het model opnieuw "getraind".

<!-- -->

<!--https://www.forbes.com/sites/bernardmarr/2016/09/30/what-are-the-top-10-use-cases-for-machine-learning-and-ai/#2318fab294c9 -->

[Medisch Contact](https://www.medischcontact.nl/nieuws/laatste-nieuws/artikel/google-voorspelt-griep.htm) schrijft bijvoorbeeld dat Google een griepgolf kan voorspellen op basis van zoektermen. Maar dat is geen causaliteit. Het is namelijk niet zo dat het zoekgedrag van mensen griep veroorzaakt. Dus als het zoekgedrag van mensen verandert als gevolg van deze berichtgeving, dan verandert het aantal griepgevallen niet. 

De verwarring komt doordat het woord voorspellen zowel in passieve, niet-causale zin gebruikt wordt, b.v. als iemand ziek is en thuiszorg krijgt, voorspel ik een hoge kans dat deze persoon ouder dan 65 is.
Maar het kan ook in actieve zin gebruikt worden: b.v. ik voorspel dat als ik de prijs van dit product verlaag met 5%, dat ik er dan 10% meer van verkoop. Dit is een "causale" voorspelling, oorzaak en gevolg.

Wij zullen in dit blog betogen dat het bij machine learning cruciaal is om onderscheid te maken tussen passief voorspellen en het voorspellen van oorzaak-gevolg effecten. En wat het betekent voor analyses om de switch te maken van voorspellen naar begrijpen en hoe de voorspelkracht van allerlei algoritmes gecombineerd kan worden met de notie van causaliteit.

# Cum hoc ergo propter hoc

<!-- correlatie is geen causatie / wat is causatie-->

Het feit dat 2 verschijnselen samen optreden, cum hoc ergo propter hoc ("met dit, dus vanwege dit"") betekent niet dat er een oorzakelijk verband is tussen 2 fenomenen. Bij causaliteit gaat het om oorzaak en gevolg. Als een bepaald fenomeen de oorzaak is van een ander fenomeen, dan kunnen we ons ook **counter factuals** voorstellen. Dus wanneer het volgen van een opleiding leidt tot meer inkomen, dan kunnen we ook voor een gegeven persoon vragen wat zijn inkomen geweest _zou zijn_ wanneer hij een andere opleiding gevolgd zou hebben.

Of in andere woorden, een causaal effect kan aangeduid worden als het verschil tussen de uitkomst wanneer een bepaalde interventie is toegepast en de uitkomst wanneer de interventie niet zou worden toegepast.  

Helaas zien de data die we krijgen er vaak als volgt uit:



```{r}
X_1 <- c("Man", "Vrouw", "Vrouw", "...")

PO <- as.data.frame(X_1) %>%
  mutate(X_2 =  c("9", "60", "7", "...")) %>%
  mutate(X_3 = c("14", "36", "2", "...")) %>%
  mutate(X_i =  c("1", "0", "1", "...")) %>%
  mutate(I = c("0", "1", "1", "...")) %>%
  mutate(Y0 = c("67", "NA", "NA", "...")) %>%
  mutate(Y1 = c("NA", "113", "54", "..."))



kable(PO)

```

Waarbij de uitkomst is weergegeven als $Y$ en $I$ staat voor het wel ($I=1$) of niet ($I=0$) doen van een interventie. $Y0$ is de uitkomst wanneer er geen interventie is gedaan en $Y1$ is de uitkomst wanneer er wel een interventie is gedaan. De $X$-en staan voor voorspellende kenmerken.

Het probleem is dat we altijd maar 1 van de potentiele uitkomsten kennen. De ontbrekende uitkomst is in de tabel weergegeven als "NA" .

Met machine learning kunnen we ontbrekende waarden vaak goed voorspellen. Daarmee kunnen we machine learning kunnen gebruiken voor het schatten van causale effecten. Om het netjes te doen zouden we overigens alle waarden voor $Y$ bij $I = 0$ en $I = 1$ moeten voorspellen.
<!-- wat bedoel j met "om het netjes te doen" -->

In dit blog willen we laten zien, dat je dat niet zo maar kunt doen. Om echt causale conclusies te kunnen trekken, moet je goed nadenken over welke variabelen je wel en niet kunt gebruiken om $Y$ gegeven de interventie $I$ te kunnen voorspellen. Met andere woorden: om een **causale** voorspelling te doen over een interventie in de wereld.

# Voorspellen versus begrijpen

Bij het voorspellen van fenomenen gaat het om het vinden van patronen ("correlaties") in de data. 

Het beschrijven en voorspellen van fenomenen kan op grond van de data alleen. Echter, om fenomenen te kunnen **verklaren** (begrijpen) is een causaal model nodig. In een causaal model wordt de kennis over oorzaken en gevolgen expliciet vastgelegd. Het causale model staat dus buiten de data en geeft weer wat de veronderstellingen zijn van de onderzoeker.

Gegeven een causaal model kunnen vragen beantwoord worden als:

* Wat zijn de oorzaken van een bepaald fenomeen?
* Welke gevolgen heeft ingrijpen op een oorzaak op het bestudeerde fenomeen?

Als we begrijpen wat een voorspelling drijft, dan kunnen we we iets doen om het verschijnsel waarin we zijn geinteresseerd te veranderen. Dat iets doen noemen we in dit blog een "interventie". Voorbeelden van interventies zijn:

- beleid formuleren
- een medische ingreep doen
- meer informatie verschaffen aan mensen
- etc

Om het probleem van het voorspellen op basis van correlaties te illustreren, hebben we een fictieve dataset gemaakt met gegevens CITO-scores van hun kinderen en het aantal boeken dat hun ouders bezit.

Uit figuur 1 blijkt dat er een correlatie is tussen het aantal boeken dat ouders bezit en de CITO-scores van hun kinderen.


```{r}
set.seed(123)

N <- 1000

IQ_groep <- rep(c(1:10), times = N/10)

conf <- as.data.frame(IQ_groep) %>%
  mutate(IQ = rnorm(n= N, mean = 75 + 5 * IQ_groep, sd = 2)) %>%
  group_by(IQ_groep) %>%
  mutate(IQ_mean = mean(IQ)) %>%
  ungroup() %>%
  mutate(Aantal_boeken = IQ_mean/3 + rnorm(n= N,  mean = 0), sd = 6) %>%
  mutate(C = 5 * IQ_mean + rnorm(n = N, mean = 0, sd = 10)) %>% # ruwe scores voor Cito
  mutate(Cito = (C - min(C))/max((C - min(C)))*50 + 500) %>% # verander ruwe scores in scores in de relevante range
  mutate(IQ_groep = as.factor(IQ_groep)) %>%
  select(-C)

ggplot(data = conf, aes(x = Aantal_boeken, y = Cito)) +
  geom_point() +
  geom_smooth(method = "lm") + 
  xlab("Aantal boeken dat ouders bezit") +
  ylab("Cito scores van hun kinderen") +
  theme_minimal()



```


Als deze correlatie een een causaal effect zou zijn, dan zou het helpen om boeken aan ouders van schoolgaande kinderen te geven. In dit geval is het aannemelijker dat er een andere variabele is, die zowel leidt tot meer boeken als tot hogere CITO-scores bij kinderen: het IQ van de ouders.

Zodra we rekening houden met ("controleren voor") het IQ van de ouders, dan verdwijnt het effect. Dat betekent dat wanneer we informatie hebben over de CITO-scores van kinderen en het IQ van hun ouders, de data over het aantal boeken geen waarde toegevoegt aan onze analyse. In onderstaande plot hebben we het IQ van de ouders in 10 even grote groepen ingedeeld. Dan zien we dat binnen die groepen het aantal boeken de CITO-scores niet meer verklaren (vlakke lijn). 



```{r}


ggplot(data = conf, aes(x = Aantal_boeken, y = Cito, color = IQ_groep)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_minimal()

```



In het kader van dit blog definieren we causaliteit als het verschijnsel dat als we een variabele veranderen **in de wereld**, dat dan een andere variabele ook verandert **als gevolg hier**. In het bovenstaande voorbeeld is het aantal boeken geen oorzaak van de CITO-scores. Als we het aantal boeken zouden veranderen, dan verandert de CITO score niet. Het IQ van de ouders is wel een oorzaak van hogere CITO-scores. In dit voorbeeld kunnen wij ons overigens niet voorstellen dat we het IQ van de ouders veranderen. We kunnen wel een counter factual beredeneren: wanneer het IQ van de ouders anders geweest zou zijn, dan zou ook de CITO score anders zijn.

<!-- probleem met dir voorbeeld is dat als je het IQ van de ouder zou veranderen, door ze een lobotomie te geven b.v. het IQ van de kinderen niet zou veranderen. -->
<!-- je kan ook zeggen dat IQ ouder niet de oorzaak is, maar het genetisch materiaal van de ouders, en het gedrag van de ouders in het verleden (nature en nurture).  IQ ouder is dan een gevolg van genetisch materiaal, net als het genetisch materiaal kind een gevolg is van genetisch mat. ouder.-->

# Grafische Causale Modellen (in de volksmond: "DAG's")

## Introductie

Het is mogelijk om een causaal model in een stelsel van wiskundige vergelijkingen weer te geven. Dit wordt snel complex. Daarom worden causale modellen ook wel visueel weergegeven als _DAG_'s: Directed Acyclic Graphs. Deze DAG's zijn een visuele weergave van een model dat alle oorzaak-gevolg relaties beschrijft. 

DAG's kunnen heel simpel zijn, maar ook complex. Elke complexe DAG kan uit 3 basisvormen worden samengesteld.  

```{r}

Label <- c("X", "Z", "Y", "X", "Z", "Y", "X", "Z", "Y") # Ik gebruik dit niet: geom_text en ggeom_text_repel plaatsen labels niet mooi

DAG <- as.data.frame(Label) %>%
  mutate(X = c(1, 2, 3, 5, 6, 7, 9, 10, 11)) %>%
  mutate(Y = c(1, 3, 1, 1, 3, 1, 1, 3, 1))

ggplot(data = DAG, aes(x = X, y = Y)) +
  geom_point(size = 3, color = "black") +
  geom_segment(x = 2, xend = 1, y = 2.9, yend = 1.1, arrow = arrow(), size = 1, color = "blue") +
  geom_segment(x = 2, xend = 3, y = 2.9, yend = 1.1, arrow = arrow(), size = 1, color = "blue") +
  geom_segment(x = 5, xend = 5.9, y = 1.1, yend = 2.9, arrow = arrow(), size = 1, color = "blue") +
  geom_segment(x = 7, xend = 6.1, y = 1.1, yend = 2.9, arrow = arrow(), size = 1, color = "blue") +
  geom_segment(x = 9, xend = 9.9, y = 1.1, yend = 2.9, arrow = arrow(), size = 1, color = "blue") +
  geom_segment(x = 10.1, xend = 11, y = 2.9, yend = 1.1, arrow = arrow(), size = 1, color = "blue") +
  annotate("text", x = 1, y = 0.8, label = "X") +
  annotate("text", x = 2, y = 3.2, label = "Z") +
  annotate("text", x = 3, y = 0.8, label = "Y") +
  annotate("text", x = 5, y = 0.8, label = "X") +
  annotate("text", x = 6, y = 3.2, label = "Z") +
  annotate("text", x = 7, y = 0.8, label = "Y") +
  annotate("text", x = 9, y = 0.8, label = "X") +
  annotate("text", x = 10, y = 3.2, label = "Z") +
  annotate("text", x = 11, y = 0.8, label = "Y") +
  annotate("text", x = 2, y = 3.5, label = "Confounder") +
  annotate("text", x = 6, y = 3.5, label = "Collider") +
  annotate("text", x = 10, y = 3.5, label = "Mediator") +
  ylim(0, 5) +
    theme_void()

```

Een DAG bestaat uit 3 onderdelen:

1.  pijlen die de richting aangeven van causale relaties
2.  variabelen (die zowel geobserveerd als niet geobserveerd kunnen zijn)
3.  ontbrekende pijlen tussen variabelen

Hierna bespreken we de 3 basisvormen met een voorbeeld op basis van gefingeerde data. Daarna laten we een meer complexe DAG zien en tenslotte laten we met een voorbeeld zien hoe een DAG behulpzaam kan zijn bij het trekken van causale conclusies.



## Confounder

We kunnen het hierboven gegeven voorbeeld over het aantal boeken dat ouders bezit en de CITO score van hun kinderen weergeven in een causaal model:

```{r}
citodag <- dagitty('dag {
IQ_ouders [pos = "1,0"]
Aantal_boeken [pos = "0,1"]
Cito_score [pos = "2,1"]

Aantal_boeken <- IQ_ouders -> Cito_score

}')



plot(citodag)
```

Zoals uit het voorbeeld met de data bleek, maak het uit of je de confounder (in het Nederlands: de gezamenlijke oorzaak) wel of niet opneemt in je analyse. In dit geval wordt er gezegd dat het pad tussen de variable $Aantal\_boeken$  en $Cito\_score$ "open" is. Dat betekent er "informatie kan stromen" tussen die variabelen, die de causale analyse verstoort. Om een causale relatie tussen het $Aantal\_boeken$  en de $Cito\_score$ te kunnen identificeren, moet het pad gesloten worden. Het pad kan gesloten worden door de variable $IQ\_ouders$ in de analyse op te nemen. In statistisch jargon wordt dan gezegd dat er "gecontroleerd" wordt voor de variabele $IQ\_ouders$. 

Uit het datavoorbeeld blijkt dat wanneer we de variabele $IQ\_ouders$ niet opnemen in de analyse, dat we dan een correlatie vinden tussen de variabelen het $Aantal\_boeken$  en de $Cito\_score$. Wanneer we de de variabele $IQ\_ouders$ wel opnemen in de analyse, verdwijnt de correlatie.

Dus wanneer we uitgaan van de DAG, dan kunnen we alleen het causale effect identificeren wanneer we de confouder meenemen. Als we geen data zouden hebben over de variabele $IQ\_ouders$, dan kunnen we dus ook geen causaal effect schatten. Ook niet met de meest geavanceerde Artificial Intelligence algorithmen!

## Collider

De tweede basisvorm wordt ook wel de "collider" genoemd. In het Nederlands zou je dit kunnen vertalen als het gezamenlijk gevolg. 

In dit voorbeeld gaan we er vanuit dat we een causale relatie willen identificeren tussen Lengte en Snelheid binnen een groep basketbalspelers. Daarbij gaan we er vanuit dat er zowel professionele basketballers zijn ("NBA") als amateurs ("geen NBA"). Spelers in de NBA zijn in het algemeen zowel snel als lang. Wanneer ze niet lang zijn, dan moeten om ze hun lengte te compenseren wel enorm snel zijn (of vice versa).

Deze informatie kunnen we weergeven in de volgende DAG:

```{r}
NBAdag <- dagitty('dag {
NBA [pos = "1,0"]
Snelheid [pos = "0,1"]
Lengte [pos = "2,1"]

Lengte -> NBA <- Snelheid

}')



plot(NBAdag)
```

We kunnen nu kijken wat er gebeurd wanneer we wel of niet de professionele status van de basketballers meenemen in onze analyse.


Wanneer we de "collider" niet meenemen in onze analyse, dan vinden we in onze data geen correlatie tussen Lengte en Snelheid binnen onze groep basketballers. 

```{r}
set.seed(1)

NBA <- data.frame(Lengte = rnorm(n = 500, mean = 1.8, sd = 0.15)) %>%
  mutate(Snelheid = rnorm(n = 500, mean = 10, sd = 3)) %>% # niet gecorreleerd
  mutate(Lengte_stand = Lengte/max(Lengte)) %>%
  mutate(Snelheid_stand = Snelheid/max(Snelheid)) %>%
  mutate(NBA = as.factor(ifelse(Lengte_stand + Snelheid_stand > 1.3,  1 , 0))) # selectie
  
ggplot(data = NBA, aes(x = Lengte, y = Snelheid)) +
  geom_point() +
  geom_smooth(method = "lm", aes(group = 1)) +
  scale_color_manual(values = c("#17408B", "#C9082A")) +
  xlab("Lengte in meters") +
  ylab("Snelheid in km per uur") +
  theme_minimal()

```

Maar wanneer we de professionele status van onze basketballers wel meenemen in de analyse, vinden we plotseling wel een verband!

Merk op dat er **selectie** plaats vindt: We **selecteren** de NBA-ers (of juist de niet-NBA-ers), en na selectie ontstaat er een correlatie die er voorheen niet was.


```{r}


ggplot(data = NBA, aes(x = Lengte, y = Snelheid, color = NBA)) +
  geom_point() +
  geom_smooth(method = "lm", aes(fill = NBA) , alpha = 0.1) +
  scale_color_manual(values = c("#17408B", "#C9082A")) +
  scale_fill_manual(values = c("#17408B", "#C9082A")) +
   xlab("Lengte in meters") +
  ylab("Snelheid in km per uur") +
  theme_minimal()
```

Ook als je een regressie zou doen, dan blijkt dat 

- geen verband is tussen lengte en snelheid
- maar er wel een verband ontstaat zodra je toevoegt dat mensen in de NBA spelen.



```{r, results = 'asis'}
regNBA <- lm(Snelheid ~ Lengte, data = NBA)
regNBA1 <- lm(Snelheid ~ Lengte + NBA, data = NBA)

stargazer(regNBA, regNBA1, header = FALSE, type = "html")
```

<!-- &nbsp; -->

Dus: je creert een correlatie waar die er niet is door een collider variabele in je analyse mee te nemen. In termen van de DAG zeggen we dat een collider de informatiestroom tussen de variabelen $Lengte$ en $Snelheid$ blokkeert. Door deze variabele mee te nemen in de analyse, deblokkeren (openen) we het pad, en ontstaan er correlaties die niet-causaal zijn.

We moeten colliders dus nooit meenemen in analyses. Wanneer we dat wel doen, dan trekken we ten onrechte de conclusie dat er een causaal verband bestaat tussen in dit geval Lengte en Snelheid bij basketballers.

<!-- Conditioning on the collider via regression analysis, stratification, experimental design, or sample selection based on values of the collider create a non-causal association between X and Y (Berkson's paradox, 1938, Mayo Clinic, epi!!!). -->


## Mediator

De derde basisvorm wordt de mediator genoemd. In dit geval beinvloedt 1 variabele een tweede variabele die op haar beurt een derde variable beinvloedt.

In het volgende gestileerde voorbeeld zijn we geinteresseerd in de causale relatie tussen het budget van bijvoorbeeld een professioneel voetbalteam en de punten die dat team haalt.

Omdat je (althans: voor zover wij weten) geen punten kunt kopen, moet een eventuele causale relatie door een tweede variable lopen. In dit geval zegt de DAG dat teams met een hoger budget zich betere spelers kunnen veroorloven. En een team met betere spelers behaalt meer punten.

```{r}
Voetbaldag <- dagitty('dag {
Teamsterkte [pos = "1,0"]
Budget [pos = "0,1"]
Punten [pos = "2,1"]

Budget -> Teamsterkte -> Punten

}')



plot(Voetbaldag)

```


Op basis van deze DAG kunnen we data genereren en deze data in een grafiek weergeven.

Ook in dit geval hebben we data op basis van deze DAG gesimuleerd en willen we laten zien wat er gebeurt wanneer de mediator (in dit geval de variabele $Teamsterkte$) wel of niet wordt opgenomen in de analyse.

Wanneer we de variable $Teamsterkte$ niet opnemen in de analyse, dan zien we een vrij sterke correlatie tussen $Teamsterkte$ en het behaalde aantal $Punten$.

```{r}
set.seed(123)
Budgetten <- rnorm(n = 100, mean = 30, sd = 12)

Vb <- as.data.frame(Budgetten) %>%
  mutate(Teamsterkte = (3 * Budgetten)/(3*mean(Budgetten)) * 100 + rnorm(n = 100, mean = 0, sd = 10)) %>%
  mutate(Hulppunten = 2 * Teamsterkte + rnorm(n= 100, mean = 0, sd = 15)) %>%
  mutate(Punten = round((Hulppunten - min(Hulppunten))/max((Hulppunten - min(Hulppunten)))*63 + 23, 0 )) %>% # punten in de relevante range
  select(- Hulppunten) %>%
  mutate(Teamsterkte_groep = as.factor(ntile(Teamsterkte, 10)))
  

ggplot(data = Vb, aes(x = Budgetten, y = Punten)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_minimal()

```

Wanneer we de variabele $Teamsterkte$ zouden toevoegen aan onze analyse, dan verdwijnt de correlatie. Dit is vergelijkbaar wat er gebeurt met de confounder.


```{r}
ggplot(data = Vb, aes(x = Budgetten, y = Punten, color = Teamsterkte_groep)) +
  geom_point() + 
  geom_smooth(method = "lm") +
  theme_minimal()
```

Ook uit de resultaten van een simpele regressie blijkt dit:

```{r, results = 'asis'}
regvb <- lm(Punten ~ Budgetten, data = Vb)
regvb1 <- lm(Punten ~ Budgetten + Teamsterkte, data = Vb)

stargazer(regvb, regvb1, header = FALSE, type = "html")

```

<!-- &nbsp; -->

Maar in dit geval willen we de variabele $Teamsterkte$ niet meenemen: We willen weten of meer budget leidt tot meer punten. Dat wil zeggen dat benieuwd zijn naar het aantal punten in het geval we het budget van het voetbalteam verhogen of verlagen. Om een antwoord op die causale vraag te kunnen geven, kan de mediator niet worden meegenomen in de analyse.

<!-- Waarom mag / kan dit niet?  -->

# Voorbeeld van een meer complexe DAG

In haar afstudeerscriptie voor haar studie economie heeft Mijntje Jansen in 2019 onderzoek gedaan naar de invloed van de samenstelling van het bestuur in de verpleeghuiszorg qua geslacht op de beleefde kwaliteit van zorg. Om deze vraag te kunnen beantwoorden heeft ze de onderstaande DAG gemaakt.

Deze DAG laat zien dat DAG's behoorlijk complex kunnen zijn. Het is bij meer complexe DAG's ingewikkeld om te bepalen welke variabelen nodig zijn om causale verbanden te kunnen identificeren. Dit wordt ook wel de "minimal adjustment set" genoemd. De R-package `dagitty` bepaalde de minimum adjustment set geautomatiseerd.

Een tweede reden om deze DAG hier te laten zien is dat dit voorbeeld illustreert dat zelfs complexe modellen zoals hier zijn weergegeven goed kunnen worden besproken met andere -niet noodzakelijkerwijs wiskundig geschoolde- experts. In dit geval heeft Mijntje haar model besproken met bestuurders van verpleeghuisinstellingen, en op basis van deze gesprekken aangepast.


```{r}
thesismodel10 <- dagitty('dag {
  GSB [pos="0,5"]
  FTE [pos="1,7"]
  DR [pos="2,7"]
  GB [pos="1,5"]
  BB [pos="1,3"]
  PRB [pos="3,5"]
  LSB [pos="4,5"]
  GM [pos="5,3"]
  PRM [pos="6,3"]
  LSM [pos="7,3"]
  FPh [pos="3,1"]
  FPc [pos="7,1"]
  SC [pos="7,7"]
  UR [pos="7,14"]  
  AF [pos="7,6"]
  BP [pos="7,15"]
  BM [pos="5,2"]
  SS [pos="7,8"]
  ST [pos="7,9"]
  AS [pos="7,10"]
  SM [pos="7,11"]
  CM [pos="7,13"]
  PQ [pos="9,5"]
  
  GSB-> GB -> PRB -> LSB -> GM -> PRM -> LSM -> PQ
  BB -> PRB
  BM -> PRM
  LSB -> LSM
  FTE -> GB
  FPh -> GB
  FPh -> FPc
  LSB -> SS
  LSB -> SM
  LSB -> ST
  LSB -> AS
  LSB -> SC
  LSB -> AF
  DR -> GB
  DR -> PRB
  LSB -> PQ
  SS -> PQ
  SM -> PQ
  ST -> PQ
  AS -> PQ
  SC -> PQ
  CM -> PQ
  UR -> PQ
  AF -> PQ
  BP -> PQ
  FPc -> PQ
  CM -> SM
}')

plot(thesismodel10)
```

<!-- Hier evt discussie over uitbreiden IQ - boeken DAG -->

<!-- TOT HIER -->

# Machine learning en causaliteit

Nu we de basisbeginselen van variabele selectie en causaliteit hebben besproken, willen we graag een voorbeeld laten zien hoe er met machine learning causale effecten geschat kunnen worden. We laten zien dat het causale model erg belangrijk is voor de conclusies.

In dit voorbeeld gaan we uit van de volgende DAG.

We hebben een aantal variabelen $V_i, 1 \leq i \leq 8$ (Zoals bijvoorbeeld leeftijd, chronische ziekten etc.) die een invloed hebben op de ziektelast die mensen ondervinden. 
Deze ziektelast heeft weer een invloed op de zorgkosten die mensen maken.

Tenslotte is er een interventie $T$. Deze interventie is heel duur (en heeft daarom een invloed op de zorgkosten). Deze interventie heeft geen invloed op de ziektelast.

```{r}


g <- dagitty('dag {
    Zorgkosten [pos="1,0"]
    Treatment [pos="0,1"]
    Ziektelast [pos="2,1"]
    V_i [pos="3,1"]
   
    
V_i -> Ziektelast -> Zorgkosten <- Treatment    

    

}')
plot(g)

```

We hebben op op basis van deze DAG data gesmimuleerd. Dat betekent dus dat er in onze data geen verband is tussen het wondermiddel ("Treatment") en de ziektelast.

Uit de DAG blijkt verder dat de variabele $Zorgkosten$ een collider is en niet zou moeten worden meegnomen in een analyse waarin we het causale effect van onze $Treatment$ op de $Ziektelast$ willen schatten.

Met behulp van machine learning kunnen we een causaal effect per persoon schatten. We gebruiken in dit geval het [Random Forest Algoritme](https://en.wikipedia.org/wiki/Random_forest), omdat Random Forest een populair algortime is.

Dat kunnen we doen met behulp van het volgende stappenplan:

1. Fit een Random Forest model op de data
2. Voorspel met het model alle uitkomsten als 
    - Treatment = 0
    - Treatment = 1
3. Bereken het treatment effect als het verschil tussen de Ziektelast wanneer de Treatment 1 is en de Ziektelast wanneer de Treatment 0 is.

We doorlopen deze procedure 2 keer:

a.  Een analyse waarbij we de ziektelast voorspellen met alle variabelen ("het verkeerde model")
b.  Een analyse waarbij we de ziektelast voorspellen met alle variabelen minus de collider $Zorgkosten$ ("het goede model")


## Summary statistics

In deze tabel zien we een samenvatting van onze dataset.


```{r, results = 'asis'}

set.seed(123)

X <- matrix(sample.int(1000, size = 1000 * 8 , replace = TRUE), nrow = 1000, ncol = 8)


d1 <- as.data.frame(X) %>%
mutate(Treatment = sample(c(0, 1), 1000, replace = T)) %>%
mutate(Ziektelast = (1 * V1 + 2 * V2 + 3 * V3 + 4 * V4 + 5 * V5 + 6 * V6)/10000 + rnorm(n = 1000, mean = 0, sd = 0.1)) %>%
mutate(Zorgkosten = 3 * Treatment + 5 * Ziektelast  + rnorm(n = 1000, mean = 0, sd = 0.1)) %>%
select(Ziektelast, Treatment, Zorgkosten, everything())



stargazer(d1, type= "html",  header = FALSE)

```


&nbsp;


## Voorspellingen Random Forest

Als we onze schattingen doen, dan kunnen we eerst kijken naar de voorspelkracht van de modellen. Daarvoor gebruiken we [$R^2$](https://www.investopedia.com/terms/r/r-squared.asp). 

Uit de onderstaande grafiek blijkt dat het "verkeerde model" veel beter voorspelt dan het "goede model". Dus wanneer we het "beste model" zouden kiezen op basis van hoe goed het model voorspelt, dan zouden we het verkeerde model kiezen. 


```{r}

set.seed(123)
tr_control <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 5)
 
 
 rf_fit_all <- train(Ziektelast ~ . ,
                 data= d1,
                 method = "ranger",
                 importance= "permutation",
                 trControl = tr_control)
 
 rf_fit <- train(Ziektelast ~ .-Zorgkosten ,
                 data= d1,
                 method = "ranger",
                 importance= "permutation",
                 trControl = tr_control)
 

dresamps <- as.data.frame(rf_fit_all$resample$Rsquared) %>%
  rename(`Model met zorgkosten`= `rf_fit_all$resample$Rsquared`) %>%
  mutate(`Model zonder zorgkosten` = rf_fit$resample$Rsquared) %>%
  gather(Model, Rsquared)

ggplot(data = dresamps, aes(x= reorder(Model, Rsquared), y = Rsquared)) + 
  geom_boxplot(width = 0.1) +
  ylab("R kwardraat") + 
  xlab(" ") +
  ylim(0.5,1) +
  theme_minimal() + 
  coord_flip()

```

## Schatting van het treatment effect

Dat we het verkeerde model kiezen als we het model selecteren op basis van voorspelkracht blijkt uit de onderstaande grafiek.

We hebben onze data zo gesimuleerd dat er geen verband bestast tussen onze $Treatment$ en de $Ziektelast$. Het "goede model" schat het door ons bepaalde nul-effect vrij precies. Het "verkeerde" model daarentegen laat ten onrechte zien dat onze $Treatment$ de $Ziektelast$ verlaagt.

```{r}

set.seed(123)

# Maak testdata aan

d1a <- d1 %>%
  select(-Treatment) %>%
  mutate(Treatment = 0) %>%
  select(Ziektelast, Treatment, Zorgkosten, everything()) 


d1b <- d1 %>%
  select(-Treatment) %>%
  mutate(Treatment = 1) %>%
  select(Ziektelast, Treatment, Zorgkosten, everything())


# Voorspel waarden met model alle variabeblen


d2 <- data.frame(T0 = predict(rf_fit_all, newdata= d1a)) %>%
  mutate(T1 = predict(rf_fit_all, newdata= d1b)) %>%
  mutate(TE = T1 - T0)


d2a <- d2 %>%
  summarise(ATE = mean(TE), Sd = sd(TE)) %>%
  mutate(Upper = ATE + 1.96 * Sd) %>%
  mutate(Lower = ATE - 1.96 * Sd) %>%
  mutate(Model = as.factor("Model met zorgkosten"))


d3 <- data.frame(T0a = predict(rf_fit, newdata = d1a)) %>%
  mutate(T1a = predict(rf_fit, newdata = d1b)) %>%
  mutate(TEa = T1a - T0a)

d3a <- d3 %>%
  summarise(ATE = mean(TEa), Sd = sd(TEa)) %>%
  mutate(Upper = ATE + 1.96 * Sd) %>%
  mutate(Lower = ATE - 1.96 * Sd) %>%
  mutate(Model = as.factor("Model zonder zorgkosten"))


d4 <- rbind(d2a, d3a)

ggplot(data = d4, aes(x = Model, y = ATE)) +
  geom_point() +
  geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.1) + 
  xlab("") +
  ylab("Gemiddeld Interventie Effect") +
  ylim(-1,1) +
  theme_minimal() +
  coord_flip()


```


```{r}
# Double machine learning

set.seed(123)

# Goede Model


z <- as.matrix(d1 %>%
  select(- Ziektelast, - Treatment, - Zorgkosten))

d <- as.matrix(d1 %>%
  select(Treatment))

y <- as.matrix(d1 %>%
  select(Ziektelast))

I <- sort(sample(1: nrow(d1), nrow(d1)/2)) # main
IC <- setdiff(1:nrow(d1), I)

# voorspelling y ~ z

model1 <- randomForest(z[I, ], y[I])
model2 <- randomForest(z[IC, ], y[IC])

G1 <- predict(model1, z[IC,])
G2 <- predict(model2, z[I,])

# voorspelling y ~ d

modeld1 <- randomForest(z[I, ], as.factor(d[I]))
modeld2 <- randomForest(z[IC, ], as.factor(d[IC]))

M1 <- predict(modeld1, z[IC,])
M2 <- predict(modeld2, z[I,])

# Bereken residuals  van het model

V1 <- d[IC] - as.numeric(as.character(M1))
V2 <- d[I] - as.numeric(as.character(M2))

# Bereken treatment effect

theta_1 <- mean(V1 * (y[IC] - G1))/ mean(V1 * d[IC])
theta_2 <- mean(V2 * (y[I] - G2))/ mean(V2 * d[I])

theta_cf <- mean(c(theta_1, theta_2))


# Foute  Model


z_1 <- as.matrix(d1 %>%
  select(- Ziektelast, - Treatment))

d_1 <- as.matrix(d1 %>%
  select(Treatment))

y_1 <- as.matrix(d1 %>%
  select(Ziektelast))

I <- sort(sample(1: nrow(d1), nrow(d1)/2)) # main
IC <- setdiff(1:nrow(d1), I)

# voorspelling y ~ z

model1_1 <- randomForest(z_1[I, ], y_1[I])
model2_1 <- randomForest(z_1[IC, ], y_1[IC])

G1_1 <- predict(model1_1, z_1[IC,])
G2_1 <- predict(model2_1, z_1[I,])

# voorspelling y ~ d

modeld1_1 <- randomForest(z_1[I, ], as.factor(d_1[I]))
modeld2_1 <- randomForest(z_1[IC, ], as.factor(d_1[IC]))

M1_1 <- predict(modeld1_1, z_1[IC,])
M2_1 <- predict(modeld2_1, z_1[I,])

# Bereken residuals  van het model

V1_1 <- d_1[IC] - as.numeric(as.character(M1_1))
V2_1 <- d_1[I] - as.numeric(as.character(M2_1))

# Bereken treatment effect

theta_1_1 <- mean(V1_1 * (y_1[IC] - G1_1))/ mean(V1_1 * d_1[IC])
theta_2_1 <- mean(V2_1 * (y_1[I] - G2_1))/ mean(V2_1 * d_1[I])

theta_cf_1 <- mean(c(theta_1_1, theta_2_1))



```



# Conclusie

In dit blog hebben we laten zien dat het essentieel is om een causaal model te maken als je causale effecten wilt schatten. Zeker wanneer er machine learning technieken worden gebruikt, is de verleiding groot om zoveel mogelijk variabelen mee te nemen. In dit blog hebben we laten zien dat het toegeven aan deze verleiding tot verkeerde conclusies kan leiden.

# Literatuur

- even checken hoe we dat goed krijgen
- Book of why
- Elwert
- Causality primer van pearl
- voorbeeld boeken komt is ontleend aan Freakanomics
- ik weet niet meer waar ons voorbeeld van de NBA vandaan komt

